---
title: "Time Series Analysis"
author: "Gabriel Vicencio, Max Dippel, Diego Alonso Larre and Miguel Rubio Garcia"
date: "11/29/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

<center> <img src = "https://memegenerator.net/img/instances/71458205.jpg"> </center>

<br>

### **Introduction:**

##### Time-Series analysis is a mathematical analysis that can predict future outputs from previous past measurements. We use time-series analysis when we have **only one variable to measure** (sales of a product for example) + timing variable. Ex: If last month I sold 20 cars and this month I have sold 25 cars, how many cars will I sell next month? My only variables are car sales and time, measured by months.

##### Thus, time-series is a series of data points organized by time order (commonly a sequence of successive equally spaced events in time). We use these time series algorithms to **create a model**, and once we have our model we can use it to predict future values.

<br>

<center> **Look for the best model -> introduce the “past” inputs -> predict future values** </center>

<br>

##### We don’t use time series analysis when the values are constant (if I sold 20 cars last month, 20 cars this month, it makes no sense to do an analysis for next month). Moreover, we can only use it **when the data is stationary**. When is the data stationary? There are 3 conditions:

* The **mean** should be constant according to the time.
* The **variance** should be equal at different time intervals from the mean: each point’s distance to the mean should be equal at equal intervals of time.
* The covariance should be equal at different time intervals.


##### If these 3 conditions are fulfilled that means that my series is stationary and that we can use time-series analysis

<br>

#### **Components of time-series analysis:**

* **General trend:** The “behavior” of our values (how are they going); are they increasing (upwards) or decreasing (downwards).
* **Seasonal component:** How many peaks do we see? Example: *We have a flying company. We know that at some times of the year when there are celebrations, more people will take a plane like on Christmas, Thanksgiving or summer. So, in a year, how many peaks do we have? If there are 3 important celebrations per year, we are going to see 3 peaks. This is called the seasonal component of the time-series analysis.*
* **Irregular fluctuation:** Random component (uncontrolled circumstances). Example: *during winter there is snow and strong winds in Boston and some flights will need to be concealed. However, these conditions are not the same every year (one year there might be a lot of and another there might be quite less).*

<br>

#### **How do we transform our data into stationary?**

* If we apply the **log function** ( log(variable) ) we will obtain equal variance.
* The mean: If we apply the command “diff” (used to **differentiate**) the mean will become constant according to time. 

<br>

##### Let's see what does this mean using the R dataset "AirPassengers" that contains the information about the number of people who took the plane per month from 1949 to 1960. To do so and future analysis, make sure you have downloaded the following packages: **nlme**, **MuMIn**, **MASS**, **ggplot2** and **curl**.

<br>

#### Loading libraries
```{r}
library(nlme) # gls (generalized least squares)
library(MuMIn) #AICc
library(MASS) # acf(), pacf(), ARIMA()
library(ggplot2) # for graphing
library(curl) # for loading the data
```

##### First let's take a look to our data:

```{r}
AirPassengers #Each value represents the number of people ("Air Passengers") that took the plane the month indicated by the column and the year indicated by the row.
```

##### Now let's plot the data:
```{r}
plot(AirPassengers)
abline(reg=lm(AirPassengers~time(AirPassengers)))
```

##### With the following line of code we can see the mean ploted as a line:
```{r}

```

##### We see the mean is increasing every year. In addition, dispersion is increasing with time, which means the variance is not equal at different time points. Thus, our data is not stationary. Let's transform our data using "log" and "dif" so we can apply on it a time-series analysis:

```{r}
plot(log(AirPassengers)) #Using "log" function to change the variance.
abline(reg=lm(log(AirPassengers)~time(log(AirPassengers)))) #Now the variance is constant according to time.
```

```{r}
plot(diff(log(AirPassengers))) #Using "diff" function to change the mean.
abline(reg=lm(diff(log(AirPassengers))~time(diff(log(AirPassengers))))) #Now the mean is constant according to time.
```

##### Now that the data is stationary we could apply different models for time series analysis like the ArIMa model (Autoregressive Integrated Moving average).

<br>

### **ARIMA Model:**

##### Now, we are going to use ARIMA models to get rid of autocorrelation in the residuals of models of a given dataset, to make our errors random and independent from each other. For this exercise, we are going to use the hypothetical case of an obsidian analysis by portable X-ray fluorescence. The samples were taken from one archaeological site that was active from AD 120 to 1520 (Classic to Postclassic) in Central Mexico. We can also use the ARIMA analysis to predict change over time of specific obsidian sources used by the people from this site. To be able to achieve this, we are going to use specific trace elements that can help us understand the changes in the different sources of obsidian and if it is possible to establish a temporal pattern.

##### The values are presented in parts per million (ppm) and represent the mean of different obsidians analyzed with XRFp from the same site with very well-dated contexts. Each value represents the mean of all obsidians analyzed with pXRF that were recovered from various contexts dating ten-year terms.

#### Loading the dataset:
```{r}
f <- curl("https://raw.githubusercontent.com/dalarre/Time-Series-Analysis/main/Dataset_R_2.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
```{r}
names(d)
```

#### Graphing the dataset
```{r}
plot(d$Rb ~ d$X.U.FEFF.Year.AD, data=d, type="l", pch=16, 
     ylab="ppm", xlab="Year")
ggplot(data = d, aes(x=d$X.U.FEFF.Year.AD, y= d$Rb)) + geom_line()
```

### Analysis of the autocorrelation in the data
#### This is the pacf() function. This stands for partial auto-correlation function which is used to find auto-correlation in the data
```{r}


?pacf()
pacf1 <- pacf(d$Rb, main="PACF")
```

#### As you can see, there is auto correlation present in the data. It is shown by the data points which go beyond the blue confidence interval. Specifically the auto-correlation is outside the confidence interval at a lag of 1 and 2. This will guide later analysis. 

### Creating a simple linear model of the data
#### The next step in the analysis is to make a simple linear model of the data. We know this model does not accurately capture the variation in the data. However, it is helpful to look at the error present in this model to confirm that auto-correlation is present. 
```{r}
lm1 <- lm(d$Rb ~ d$X.U.FEFF.Year.AD, data=d)
summary (lm1)
```

### Analyzing autocorrelation in the residuals 
#### After that, we will extract the residuals (error) from this model and see if this error is auto-correlated as well
```{r}


d$mod.resid <- residuals(lm1)
pacf2 <- pacf(d$mod.resid, main="PACF - Rb residuals")
pacf2

```

#### Residuals should be random. The values in this graph which are outside the confidence interval tell us that the residuals are not random. The goal of this analysis will be to correct for these non-random residuals.


### Creating ARIMA Models
#### ARIMA stands for autoregressive intergrated moving average models. The values p, d and q are different values which help describe the data we are trying to fit to the ARIMA model. 
```{r}
### The arima() function in r is the model we will use to fit the data based on the parameters predicted by the pacf(). Specifically, since the values outside the confidence interval were a lag term of 1 and 2 we will use auto-regressive terms and moving average terms near 1-2. In general, it is good to expand a little beyond what is beyond the confidence interval because pacf() is not perfect. 

#AR
ar.1 <- arima(d$mod.resid, order=c(1,0,0)) 
ar.2 <- arima(d$mod.resid, order=c(2,0,0))
ar.3 <- arima(d$mod.resid, order=c(3,0,0))
ar.4 <- arima(d$mod.resid, order=c(4,0,0))

# MA
ma.1 <- arima(d$mod.resid, order=c(0,0,1)) 
ma.2 <- arima(d$mod.resid, order=c(0,0,2)) 
ma.3 <- arima(d$mod.resid, order=c(0,0,3)) 
ma.4 <- arima(d$mod.resid, order=c(0,0,4)) 

```

### Challenge 1: rank the ARIMA models using AICc
#### After fitting multiple ARIMAs we will use AICc() to rank the models from best to worst. The lower the AICc value the better it is 
```{r}


AICc(ar.1)
AICc(ar.2)
AICc(ar.3)
AICc(ar.4)
AICc(ma.1)
AICc(ma.2)
AICc(ma.3)
AICc(ma.4)
```
#### After running the AICc() we found that the models with 2 autoregressive terms and 3 moving average terms are the best fitted to the data. 

### Creating linear models which account for non-random error
#### We will now use the gls() function to model these two best fitting parameters into a linear model. gls stands for generalized least squares model. This type of linear model allows us to incorporate ARIMA terms into the modeling. 
```{r}


gls.mod1 <- gls(Rb ~ X.U.FEFF.Year.AD, data=d, correlation = corARMA(p=2, q=0))
gls.mod2 <- gls(Rb ~ X.U.FEFF.Year.AD, data=d, correlation = corARMA(p=0, q=3))
```

### Challenge 2: rank the models using AICc
```{r}
AICc(gls.mod1)
AICc(gls.mod2)
```
#### While the AICc value for model 1 is less than the model 2, they are so close we can consider than equivanlent. This means the data is either best modeled with 2 autoregressive terms or 3 moving average terms.  


### Checking our work  
#### Let's make a summary for the first linear model and see if the residuals still have auto-correlation using pacf()
```{r}
summary(gls.mod1)
confint(gls.mod1)

d$new.residuals <- resid(gls.mod1, type="normalized")

pacf3 <- pacf(d$new.residuals, main="PACF - GLS model residuals")
```

####  Poof! The auto-correlation is gone! Notice how there is now no values outside the confidence intervals. That means the data has been modeled in such a way that there is no autocorrelation error that we are not accounting for in the way we model the data. 

####  To make sure that the model is appropriate, we check if the residuals are normally distributed using a qq-plot

```{r}
qqnorm(d$new.residuals)
qqline(d$new.residuals)
```
####  The residuals adjust pretty well to the line in the Q-Q plot, which means they are normally distributed.


